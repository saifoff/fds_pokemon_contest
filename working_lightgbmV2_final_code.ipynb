{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTEBOOK SUMMARY\n",
    "\n",
    "The first 3 blocks of code show the preprocessing and model running used for the final submissions.\n",
    "Subsequent code shows discarded approaches, whicha are touched upon in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBRARY LOADING\n",
    "\n",
    "This script loads all required libraries \n",
    "It sets up file paths for the dataset, then loads both the training and test .jsonl files line by line into Python lists.\n",
    "After loading, it reports how many battles were successfully read.\n",
    "Errors during loading are caught and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'C:\\Users\\ivayl\\Documents\\GitHub\\FDS-Pok-mon-Battle\\fds-pokemon-battles-prediction-2025\\train.jsonl'...\n",
      "Loading data from 'C:\\Users\\ivayl\\Documents\\GitHub\\FDS-Pok-mon-Battle\\fds-pokemon-battles-prediction-2025\\test.jsonl'...\n",
      "Successfully loaded 10000 battles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LIBRARY LOADING\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# XGBoost & LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Define the path to our data ---\n",
    "COMPETITION_NAME = 'fds-pokemon-battles-prediction-2025'\n",
    "DATA_PATH = os.path.join('../input', COMPETITION_NAME)\n",
    "\n",
    "train_file_path = os.path.join(DATA_PATH, 'train.jsonl')\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "printing = False\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "try:\n",
    "    print(f\"Loading data from '{train_file_path}'...\")\n",
    "    with open(train_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Loading data from '{test_file_path}'...\")\n",
    "    with open(test_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "\n",
    "    print(f\"Successfully loaded {len(train_data)} battles.\\n\")\n",
    "    if printing: \n",
    "        # Inspect the first few battles\n",
    "        for i, battle in enumerate(train_data[:5]):  # first 3 battles\n",
    "            print(f\"--- Battle {i+1} ---\")\n",
    "            print(\"Keys:\", list(battle.keys()))\n",
    "            \n",
    "            # Show top-level values\n",
    "            for key, value in battle.items():\n",
    "                if key == 'battle_timeline':\n",
    "                    print(f\"{key}:\")\n",
    "                    for turn in value[:3]:  # show first 3 turns\n",
    "                        print(\"   \", turn)\n",
    "                    if len(value) > 3:\n",
    "                        print(\"   ... (more turns)\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"Error loading data:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING EXPLANATION\n",
    "\n",
    "This function takes raw battle dictionaries and converts them into a structured DataFrame.\n",
    "It processes every battle one at a time, and loops through all turns inside each battle.\n",
    "It extracts turn-level information such as Pokémon names, HP percentages, statuses, boosts, and move details.\n",
    "It also attaches the overall battle outcome when processing training data.\n",
    "Each turn becomes a single row in the output table, and all these rows are gathered into a list.\n",
    "After processing all battles, the function converts the list into a Pandas DataFrame.\n",
    "Several columns containing categorical values, such as Pokémon names and move types, are one-hot encoded.\n",
    "The function returns this fully encoded DataFrame.\n",
    "\n",
    "The script then calls the function twice.\n",
    "The first call processes the training dataset and prints the resulting shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98deb797d1c4b509a944ac84e6c84a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing battles:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame shape: (300000, 187)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b98f0d42853470187e0a48fd30074d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing battles:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame shape: (150000, 186)\n"
     ]
    }
   ],
   "source": [
    "def process_battle_data(data, is_train=True):\n",
    "    all_rows = []\n",
    "    for battle in tqdm(data, desc=\"Processing battles\"):\n",
    "        battle_id = battle['battle_id']\n",
    "        player_won = int(battle['player_won']) if is_train else None\n",
    "        \n",
    "        for turn in battle['battle_timeline']:\n",
    "            turn_num = turn['turn']\n",
    "            \n",
    "            p1_state = turn['p1_pokemon_state']\n",
    "            p2_state = turn['p2_pokemon_state']\n",
    "            p1_move = turn.get('p1_move_details') or {}\n",
    "            p2_move = turn.get('p2_move_details') or {}\n",
    "\n",
    "            # === Construct turn-level record ===\n",
    "            row = {\n",
    "                'battle_id': battle_id,\n",
    "                'turn': turn_num,\n",
    "\n",
    "                # Pokémon identity\n",
    "                'p1_pokemon_name': p1_state.get('name', None),\n",
    "                'p2_pokemon_name': p2_state.get('name', None),\n",
    "\n",
    "                # HP, status, and effects\n",
    "                'p1_hp_pct': p1_state.get('hp_pct', np.nan),\n",
    "                'p2_hp_pct': p2_state.get('hp_pct', np.nan),\n",
    "                'p1_status': p1_state.get('status', 'nostatus'),\n",
    "                'p2_status': p2_state.get('status', 'nostatus'),\n",
    "\n",
    "                # Moves\n",
    "                'p1_move_name': p1_move.get('name', None),\n",
    "                'p1_move_type': p1_move.get('type', None),\n",
    "                'p1_move_power': p1_move.get('base_power', 0),\n",
    "\n",
    "                'p2_move_name': p2_move.get('name', None),\n",
    "                'p2_move_type': p2_move.get('type', None),\n",
    "                'p2_move_power': p2_move.get('base_power', 0),\n",
    "\n",
    "                # Boosts\n",
    "                'p1_boost_atk': p1_state.get('boosts', {}).get('atk', 0),\n",
    "                'p1_boost_def': p1_state.get('boosts', {}).get('def', 0),\n",
    "                'p1_boost_spa': p1_state.get('boosts', {}).get('spa', 0),\n",
    "                'p1_boost_spd': p1_state.get('boosts', {}).get('spd', 0),\n",
    "                'p1_boost_spe': p1_state.get('boosts', {}).get('spe', 0),\n",
    "\n",
    "                'p2_boost_atk': p2_state.get('boosts', {}).get('atk', 0),\n",
    "                'p2_boost_def': p2_state.get('boosts', {}).get('def', 0),\n",
    "                'p2_boost_spa': p2_state.get('boosts', {}).get('spa', 0),\n",
    "                'p2_boost_spd': p2_state.get('boosts', {}).get('spd', 0),\n",
    "                'p2_boost_spe': p2_state.get('boosts', {}).get('spe', 0),\n",
    "            }\n",
    "\n",
    "            if is_train:\n",
    "                row['player_won'] = player_won\n",
    "\n",
    "            all_rows.append(row)\n",
    "\n",
    "    # === Create DataFrame ===\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # === One-hot encode categorical features ===\n",
    "    categorical_cols = [\n",
    "        'p1_pokemon_name', 'p2_pokemon_name',\n",
    "        'p1_status', 'p2_status',\n",
    "        'p1_move_type', 'p2_move_type',\n",
    "        'p1_move_name', 'p2_move_name'\n",
    "    ]\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = process_battle_data(train_data, is_train=True)\n",
    "print(\"Train DataFrame shape:\", train_df.shape)\n",
    "\n",
    "test_df = process_battle_data(test_data, is_train=False)\n",
    "print(\"Test DataFrame shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression — Train Acc: 0.841, Val Acc: 0.842, F1: 0.843, AUC: 0.894\n",
      "Random Forest — Train Acc: 0.911, Val Acc: 0.815, F1: 0.818, AUC: 0.877\n",
      "Gradient Boosting — Train Acc: 0.860, Val Acc: 0.828, F1: 0.830, AUC: 0.896\n",
      "XGBoost — Train Acc: 1.000, Val Acc: 0.833, F1: 0.834, AUC: 0.895\n",
      "[LightGBM] [Info] Number of positive: 3988, number of negative: 4012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10076\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498500 -> initscore=-0.006000\n",
      "[LightGBM] [Info] Start training from score -0.006000\n",
      "LightGBM — Train Acc: 0.981, Val Acc: 0.829, F1: 0.831, AUC: 0.898\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "                 Model  Train Accuracy  Val Accuracy  F1-Score   ROC-AUC\n",
      "0  Logistic Regression        0.841000        0.8420  0.843409  0.893680\n",
      "1        Random Forest        0.911250        0.8145  0.818048  0.876510\n",
      "2    Gradient Boosting        0.859875        0.8280  0.830375  0.895790\n",
      "3              XGBoost        0.999875        0.8325  0.833582  0.894664\n",
      "4             LightGBM        0.980500        0.8290  0.831361  0.897581\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# === BUILD POKEMON DATABASE FROM TRAIN DATA ==========================\n",
    "# =====================================================================\n",
    "\n",
    "def build_pokemon_database(battles_data):\n",
    "    all_rows = []\n",
    "    for battle in battles_data:\n",
    "        battle_id = battle['battle_id']\n",
    "        p1_team = battle.get('p1_team_details', [])\n",
    "        p2_team = battle.get('p2_team_details', [])\n",
    "        \n",
    "        for team, player in zip([p1_team, p2_team], ['p1','p2']):\n",
    "            for pokemon in team:\n",
    "                row = {\n",
    "                    'battle_id': battle_id,\n",
    "                    f'{player}_pokemon_name': pokemon.get('name'),\n",
    "                    f'{player}_level': pokemon.get('level'),\n",
    "                    f'{player}_base_hp': pokemon.get('base_hp'),\n",
    "                    f'{player}_base_atk': pokemon.get('base_atk'),\n",
    "                    f'{player}_base_def': pokemon.get('base_def'),\n",
    "                    f'{player}_base_spa': pokemon.get('base_spa'),\n",
    "                    f'{player}_base_spd': pokemon.get('base_spd'),\n",
    "                    f'{player}_base_spe': pokemon.get('base_spe'),\n",
    "                    f'{player}_type1': pokemon.get('types')[0] if len(pokemon.get('types', []))>0 else None,\n",
    "                    f'{player}_type2': pokemon.get('types')[1] if len(pokemon.get('types', []))>1 else None\n",
    "                }\n",
    "                all_rows.append(row)\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "# =====================================================================\n",
    "# === MERGE P2 TEAM STATS INTO MAIN TRAIN DF ==========================\n",
    "# =====================================================================\n",
    "\n",
    "pokemon_db = build_pokemon_database(train_data)\n",
    "\n",
    "p2_cols = [c for c in pokemon_db.columns if c.startswith('p2')]\n",
    "p2_stats_df = pokemon_db[['battle_id'] + p2_cols].groupby('battle_id').mean().reset_index()\n",
    "\n",
    "train_df_extended = train_df.merge(p2_stats_df, on='battle_id', how='left')\n",
    "\n",
    "# =====================================================================\n",
    "# === FEATURE SELECTION ================================================\n",
    "# =====================================================================\n",
    "\n",
    "feature_cols = [c for c in train_df_extended.columns if c not in ['battle_id', 'turn', 'player_won']]\n",
    "\n",
    "# =====================================================================\n",
    "# === SCALE FEATURES ===================================================\n",
    "# =====================================================================\n",
    "\n",
    "X = train_df_extended[feature_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y = train_df_extended.groupby('battle_id')['player_won'].first().values\n",
    "\n",
    "# =====================================================================\n",
    "# === BATTLE-LEVEL FEATURE ENGINEERING ================================\n",
    "# =====================================================================\n",
    "\n",
    "def build_chunk_features(df, feature_cols):\n",
    "    out = []\n",
    "    for bid in df['battle_id'].unique():\n",
    "        battle = df[df['battle_id'] == bid]\n",
    "        feats = []\n",
    "        for (start, end) in [(1,10),(11,20),(21,30)]:\n",
    "            subset = battle[(battle['turn']>=start)&(battle['turn']<=end)]\n",
    "            feats.extend(subset[feature_cols].mean().values if not subset.empty else [0.0]*len(feature_cols))\n",
    "        out.append(feats)\n",
    "    return np.array(out)\n",
    "\n",
    "def build_partial_features(df, hp_cols, atk_cols, def_cols):\n",
    "    out = []\n",
    "    intervals = [(1,10),(11,20),(21,25),(26,30)]\n",
    "    groups = [hp_cols, atk_cols, def_cols]\n",
    "\n",
    "    for bid in df['battle_id'].unique():\n",
    "        battle = df[df['battle_id'] == bid]\n",
    "        feats = []\n",
    "        for cols in groups:\n",
    "            for (start,end) in intervals:\n",
    "                subset = battle[(battle['turn']>=start)&(battle['turn']<=end)]\n",
    "                feats.extend(subset[cols].mean().values if not subset.empty else [0.0]*len(cols))\n",
    "        out.append(feats)\n",
    "    return np.array(out)\n",
    "\n",
    "hp_cols  = [c for c in feature_cols if 'hp'  in c.lower()]\n",
    "atk_cols = [c for c in feature_cols if 'atk' in c.lower()]\n",
    "def_cols = [c for c in feature_cols if 'def' in c.lower()]\n",
    "\n",
    "X_chunk   = build_chunk_features(train_df_extended, feature_cols)\n",
    "X_partial = build_partial_features(train_df_extended, hp_cols, atk_cols, def_cols)\n",
    "X_combined = np.concatenate([X_chunk, X_partial], axis=1)\n",
    "\n",
    "# =====================================================================\n",
    "# === TRAIN/VAL SPLIT ==================================================\n",
    "# =====================================================================\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# === MODELS ===========================================================\n",
    "# =====================================================================\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "all_models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\": gb,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"LightGBM\": lgbm_model\n",
    "}\n",
    "\n",
    "# =====================================================================\n",
    "# === TRAIN & EVALUATE ================================================\n",
    "# =====================================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in all_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_prob = model.predict_proba(X_val)[:,1] if hasattr(model,\"predict_proba\") else y_val_pred\n",
    "    \n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    auc = roc_auc_score(y_val, y_val_prob)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train Accuracy\": train_acc,\n",
    "        \"Val Accuracy\": val_acc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": auc\n",
    "    })\n",
    "    \n",
    "    print(f\"{name} — Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL PIPELINE SUMMARY\n",
    "\n",
    "This code builds the full training and inference pipeline for predicting battle outcomes.\n",
    "It extracts Pokémon base stats and types from each battle and merges them into the main dataset.\n",
    "It identifies all usable features and creates additional engineered features by averaging turn-level values across multiple battle segments.\n",
    "These chunk-based and partial-segment features are combined into one final feature matrix.\n",
    "The data is split into training and validation sets.\n",
    "Several models are built, including Random Forests, Gradient Boosting, XGBoost, and LightGBM.\n",
    "Multiple voting ensembles are constructed and trained on the engineered features.\n",
    "The same processing pipeline is applied to the test set.\n",
    "Each ensemble outputs a separate CSV file containing the final predictions.\n",
    "\n",
    "The final model settings were chosen after grid-search experimentation and repeated back-and-forth testing of different feature-processing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression — Train Acc: 0.841, Val Acc: 0.842\n",
      "Random_Forest — Train Acc: 0.911, Val Acc: 0.815\n",
      "Gradient_Boosting — Train Acc: 0.860, Val Acc: 0.829\n",
      "XGBoost — Train Acc: 1.000, Val Acc: 0.833\n",
      "[LightGBM] [Info] Number of positive: 3988, number of negative: 4012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10076\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498500 -> initscore=-0.006000\n",
      "[LightGBM] [Info] Start training from score -0.006000\n",
      "LightGBM — Train Acc: 0.981, Val Acc: 0.829\n",
      "[LightGBM] [Info] Number of positive: 3988, number of negative: 4012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10076\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498500 -> initscore=-0.006000\n",
      "[LightGBM] [Info] Start training from score -0.006000\n",
      "Ensemble_Soft — Train Acc: 0.970, Val Acc: 0.832\n",
      "[LightGBM] [Info] Number of positive: 3988, number of negative: 4012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10076\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498500 -> initscore=-0.006000\n",
      "[LightGBM] [Info] Start training from score -0.006000\n",
      "Ensemble_Hard — Train Acc: 0.950, Val Acc: 0.830\n",
      "Ensemble_RF_XGB — Train Acc: 0.996, Val Acc: 0.833\n",
      "[LightGBM] [Info] Number of positive: 3988, number of negative: 4012\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10076\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 542\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498500 -> initscore=-0.006000\n",
      "[LightGBM] [Info] Start training from score -0.006000\n",
      "Ensemble_GB_LGB — Train Acc: 0.928, Val Acc: 0.829\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_pokemon_database(battles_data):\n",
    "    all_rows = []\n",
    "    for battle in battles_data:\n",
    "        battle_id = battle['battle_id']\n",
    "        p1_team = battle.get('p1_team_details', [])\n",
    "        p2_team = battle.get('p2_team_details', [])\n",
    "        for team, player in zip([p1_team, p2_team], ['p1','p2']):\n",
    "            for pokemon in team:\n",
    "                row = {\n",
    "                    'battle_id': battle_id,\n",
    "                    f'{player}_pokemon_name': pokemon.get('name'),\n",
    "                    f'{player}_level': pokemon.get('level'),\n",
    "                    f'{player}_base_hp': pokemon.get('base_hp'),\n",
    "                    f'{player}_base_atk': pokemon.get('base_atk'),\n",
    "                    f'{player}_base_def': pokemon.get('base_def'),\n",
    "                    f'{player}_base_spa': pokemon.get('base_spa'),\n",
    "                    f'{player}_base_spd': pokemon.get('base_spd'),\n",
    "                    f'{player}_base_spe': pokemon.get('base_spe'),\n",
    "                    f'{player}_type1': pokemon.get('types')[0] if len(pokemon.get('types', []))>0 else None,\n",
    "                    f'{player}_type2': pokemon.get('types')[1] if len(pokemon.get('types', []))>1 else None\n",
    "                }\n",
    "                all_rows.append(row)\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "pokemon_db = build_pokemon_database(train_data)\n",
    "p2_cols = [c for c in pokemon_db.columns if c.startswith('p2')]\n",
    "p2_stats_df = pokemon_db[['battle_id'] + p2_cols].groupby('battle_id').mean().reset_index()\n",
    "train_df_extended = train_df.merge(p2_stats_df, on='battle_id', how='left')\n",
    "\n",
    "feature_cols = [c for c in train_df_extended.columns if c not in ['battle_id', 'turn', 'player_won']]\n",
    "X = train_df_extended[feature_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = train_df_extended.groupby('battle_id')['player_won'].first().values\n",
    "\n",
    "def build_chunk_features(df, feature_cols):\n",
    "    out = []\n",
    "    for bid in df['battle_id'].unique():\n",
    "        battle = df[df['battle_id'] == bid]\n",
    "        feats = []\n",
    "        for (start, end) in [(1,10),(11,20),(21,30)]:\n",
    "            subset = battle[(battle['turn']>=start)&(battle['turn']<=end)]\n",
    "            feats.extend(subset[feature_cols].mean().values if not subset.empty else [0.0]*len(feature_cols))\n",
    "        out.append(feats)\n",
    "    return np.array(out)\n",
    "\n",
    "def build_partial_features(df, hp_cols, atk_cols, def_cols):\n",
    "    out = []\n",
    "    intervals = [(1,10),(11,20),(21,25),(26,30)]\n",
    "    groups = [hp_cols, atk_cols, def_cols]\n",
    "    for bid in df['battle_id'].unique():\n",
    "        battle = df[df['battle_id'] == bid]\n",
    "        feats = []\n",
    "        for cols in groups:\n",
    "            for (start,end) in intervals:\n",
    "                subset = battle[(battle['turn']>=start)&(battle['turn']<=end)]\n",
    "                feats.extend(subset[cols].mean().values if not subset.empty else [0.0]*len(cols))\n",
    "        out.append(feats)\n",
    "    return np.array(out)\n",
    "\n",
    "hp_cols  = [c for c in feature_cols if 'hp'  in c.lower()]\n",
    "atk_cols = [c for c in feature_cols if 'atk' in c.lower()]\n",
    "def_cols = [c for c in feature_cols if 'def' in c.lower()]\n",
    "\n",
    "X_chunk   = build_chunk_features(train_df_extended, feature_cols)\n",
    "X_partial = build_partial_features(train_df_extended, hp_cols, atk_cols, def_cols)\n",
    "X_combined = np.concatenate([X_chunk, X_partial], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define all models including simple ones\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, max_depth=8, learning_rate=0.05, tree_method=\"hist\", eval_metric=\"logloss\")\n",
    "lgbm_model = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "ensemble_soft = VotingClassifier([(\"rf\", rf), (\"gb\", gb), (\"xgb\", xgb_model), (\"lgb\", lgbm_model)], voting=\"soft\")\n",
    "ensemble_hard = VotingClassifier([(\"rf\", rf), (\"gb\", gb), (\"xgb\", xgb_model), (\"lgb\", lgbm_model)], voting=\"hard\")\n",
    "ensemble_rf_xgb = VotingClassifier([(\"rf\", rf), (\"xgb\", xgb_model)], voting=\"soft\")\n",
    "ensemble_gb_lgb = VotingClassifier([(\"gb\", gb), (\"lgb\", lgbm_model)], voting=\"soft\")\n",
    "\n",
    "all_models = {\n",
    "    \"Logistic_Regression\": log_reg,\n",
    "    \"Random_Forest\": rf,\n",
    "    \"Gradient_Boosting\": gb,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"LightGBM\": lgbm_model,\n",
    "    \"Ensemble_Soft\": ensemble_soft,\n",
    "    \"Ensemble_Hard\": ensemble_hard,\n",
    "    \"Ensemble_RF_XGB\": ensemble_rf_xgb,\n",
    "    \"Ensemble_GB_LGB\": ensemble_gb_lgb\n",
    "}\n",
    "\n",
    "# Train all models and print train/val accuracy\n",
    "for name, model in all_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"{name} — Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "# Prepare test set\n",
    "pokemon_db_test = build_pokemon_database(test_data)\n",
    "p2_cols_test = [c for c in pokemon_db_test.columns if c.startswith('p2')]\n",
    "p2_stats_test = pokemon_db_test[['battle_id'] + p2_cols_test].groupby('battle_id').mean().reset_index()\n",
    "test_df_extended = test_df.merge(p2_stats_test, on='battle_id', how='left')\n",
    "\n",
    "X_chunk_test = build_chunk_features(test_df_extended, feature_cols)\n",
    "X_partial_test = build_partial_features(test_df_extended, hp_cols, atk_cols, def_cols)\n",
    "X_test_combined = np.concatenate([X_chunk_test, X_partial_test], axis=1)\n",
    "\n",
    "# Save predictions for all models\n",
    "for name, model in all_models.items():\n",
    "    preds = model.predict(X_test_combined)\n",
    "    out_df = pd.DataFrame({\n",
    "        \"battle_id\": test_df_extended['battle_id'].unique(),\n",
    "        \"prediction\": preds\n",
    "    })\n",
    "    out_df.to_csv(f\"{name}_FINAL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH (SETUP FOR LOGISTIC REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = [\n",
    "    {\"penalty\": \"l2\", \"solver\": \"lbfgs\", \"C\": C, \"class_weight\": cw}\n",
    "    for C in [0.01, 0.1, 1, 3, 10]\n",
    "    for cw in [None, \"balanced\"]\n",
    "] + [\n",
    "    {\"penalty\": \"l1\", \"solver\": \"liblinear\", \"C\": C, \"class_weight\": None}\n",
    "    for C in [0.1, 1, 3]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_grid:\n",
    "    model = LogisticRegression(\n",
    "        C=params[\"C\"],\n",
    "        penalty=params[\"penalty\"],\n",
    "        solver=params[\"solver\"],\n",
    "        class_weight=params[\"class_weight\"],\n",
    "        max_iter=5000\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred   = model.predict(X_val)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc   = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"TRY → {params} | Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"params\": params,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"model\": model\n",
    "    })\n",
    "\n",
    "# Pick best 3 models\n",
    "results_sorted = sorted(results, key=lambda x: x[\"val_acc\"], reverse=True)\n",
    "top3 = results_sorted[:3]\n",
    "\n",
    "print(\"\\n=== TOP 3 MODELS ===\")\n",
    "for r in top3:\n",
    "    print(r[\"params\"], \" | Val Acc =\", r[\"val_acc\"])\n",
    "\n",
    "# ============================================================\n",
    "# === 3) RETRAIN TOP 3 ON FULL TRAIN DATA =====================\n",
    "# ============================================================\n",
    "\n",
    "trained_models = []\n",
    "for idx, r in enumerate(top3):\n",
    "    p = r[\"params\"]\n",
    "    print(f\"\\nRetraining model #{idx+1} on FULL TRAIN: {p}\")\n",
    "\n",
    "    m = LogisticRegression(\n",
    "        C=p[\"C\"],\n",
    "        penalty=p[\"penalty\"],\n",
    "        solver=p[\"solver\"],\n",
    "        class_weight=p[\"class_weight\"],\n",
    "        max_iter=5000\n",
    "    )\n",
    "    m.fit(X_combined, y)\n",
    "    trained_models.append((idx+1, m, p))\n",
    "\n",
    "# ============================================================\n",
    "# === 4) PREPROCESS TEST DATA SAME WAY ========================\n",
    "# ============================================================\n",
    "\n",
    "# Must match your existing preprocessing exactly.\n",
    "# Assuming you already have: test_data, process_battle_data(), etc.\n",
    "\n",
    "test_df = process_battle_data(test_data, is_train=False)\n",
    "\n",
    "test_df = test_df.sort_values([\"battle_id\",\"turn\"])\n",
    "test_df_extended = test_df.merge(p2_stats_df, on=\"battle_id\", how=\"left\")\n",
    "\n",
    "X_chunk_test   = build_chunk_features(test_df_extended, feature_cols)\n",
    "X_partial_test = build_partial_features(test_df_extended, hp_cols, atk_cols, def_cols)\n",
    "X_test_combined = np.concatenate([X_chunk_test, X_partial_test], axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# === 5) RUN TOP 3 MODELS ON TEST & SAVE CSVs ================\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for rank, model, params in trained_models:\n",
    "    preds = model.predict(X_test_combined)\n",
    "\n",
    "    out_path = f\"logreg_best{rank}.csv\"\n",
    "    pd.DataFrame({\n",
    "        \"battle_id\": test_df_extended[\"battle_id\"].unique(),\n",
    "        \"predicted_win\": preds\n",
    "    }).to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved {out_path} | Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING TEAM 2'S POKEMON STATS\n",
    "\n",
    "Done using a created database of Pokemon stats (from P1's team stats), and artificially creating a team 2 stats, using the pokemons used in the first 30 moves.\n",
    "Surprisingly didn't improve performance and therefore was not used in final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pokemon_database(battles_data):\n",
    "    all_rows = []\n",
    "\n",
    "    for battle in tqdm(battles_data, desc=\"Building Pokémon database\"):\n",
    "        battle_id = battle['battle_id']\n",
    "        p1_team = battle.get('p1_team_details', [])\n",
    "\n",
    "        for pokemon in p1_team:\n",
    "            row = {\n",
    "                'battle_id': battle_id,\n",
    "                'pokemon_name': pokemon.get('name'),\n",
    "                'level': pokemon.get('level'),\n",
    "                'base_hp': pokemon.get('base_hp'),\n",
    "                'base_atk': pokemon.get('base_atk'),\n",
    "                'base_def': pokemon.get('base_def'),\n",
    "                'base_spa': pokemon.get('base_spa'),\n",
    "                'base_spd': pokemon.get('base_spd'),\n",
    "                'base_spe': pokemon.get('base_spe'),\n",
    "                'type1': pokemon.get('types')[0] if len(pokemon.get('types', [])) > 0 else None,\n",
    "                'type2': pokemon.get('types')[1] if len(pokemon.get('types', [])) > 1 else None\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_battle_data_2(data, pokemon_db, is_train=True, max_moves=30):\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for battle in tqdm(data, desc=\"Processing battles\"):\n",
    "        battle_id = battle['battle_id']\n",
    "        player_won = int(battle['player_won']) if is_train else None\n",
    "        \n",
    "        row = {'battle_id': battle_id}\n",
    "        if is_train:\n",
    "            row['player_won'] = player_won\n",
    "\n",
    "        # --- P1 team stats ---\n",
    "        p1_team = battle.get('p1_team_details', [])\n",
    "        for i, pokemon in enumerate(p1_team):\n",
    "            prefix = f'p1_pokemon{i+1}'\n",
    "            row[f'{prefix}_name'] = pokemon.get('name')\n",
    "            row[f'{prefix}_level'] = pokemon.get('level')\n",
    "            row[f'{prefix}_base_hp'] = pokemon.get('base_hp')\n",
    "            row[f'{prefix}_base_atk'] = pokemon.get('base_atk')\n",
    "            row[f'{prefix}_base_def'] = pokemon.get('base_def')\n",
    "            row[f'{prefix}_base_spa'] = pokemon.get('base_spa')\n",
    "            row[f'{prefix}_base_spd'] = pokemon.get('base_spd')\n",
    "            row[f'{prefix}_base_spe'] = pokemon.get('base_spe')\n",
    "            types = pokemon.get('types', [])\n",
    "            row[f'{prefix}_type1'] = types[0] if len(types) > 0 else None\n",
    "            row[f'{prefix}_type2'] = types[1] if len(types) > 1 else None\n",
    "\n",
    "        # --- P2 team stats (from first max_moves Pokémon used) ---\n",
    "        p2_pokemon_seen = set()\n",
    "        moves_scanned = 0\n",
    "        for turn in battle['battle_timeline']:\n",
    "            if moves_scanned >= max_moves:\n",
    "                break\n",
    "            p2_name = turn['p2_pokemon_state'].get('name')\n",
    "            if p2_name and p2_name not in p2_pokemon_seen:\n",
    "                p2_pokemon_seen.add(p2_name)\n",
    "            moves_scanned += 1\n",
    "\n",
    "        for i, name in enumerate(list(p2_pokemon_seen)):\n",
    "            prefix = f'p2_pokemon{i+1}'\n",
    "            stats = pokemon_db[pokemon_db['pokemon_name'] == name]\n",
    "            if not stats.empty:\n",
    "                stats = stats.iloc[0]  # take first occurrence\n",
    "                row[f'{prefix}_name'] = name\n",
    "                row[f'{prefix}_level'] = stats['level']\n",
    "                row[f'{prefix}_base_hp'] = stats['base_hp']\n",
    "                row[f'{prefix}_base_atk'] = stats['base_atk']\n",
    "                row[f'{prefix}_base_def'] = stats['base_def']\n",
    "                row[f'{prefix}_base_spa'] = stats['base_spa']\n",
    "                row[f'{prefix}_base_spd'] = stats['base_spd']\n",
    "                row[f'{prefix}_base_spe'] = stats['base_spe']\n",
    "                row[f'{prefix}_type1'] = stats['type1']\n",
    "                row[f'{prefix}_type2'] = stats['type2']\n",
    "            else:\n",
    "                # Fill with NaNs if Pokémon not found\n",
    "                row[f'{prefix}_name'] = name\n",
    "                row[f'{prefix}_level'] = np.nan\n",
    "                row[f'{prefix}_base_hp'] = np.nan\n",
    "                row[f'{prefix}_base_atk'] = np.nan\n",
    "                row[f'{prefix}_base_def'] = np.nan\n",
    "                row[f'{prefix}_base_spa'] = np.nan\n",
    "                row[f'{prefix}_base_spd'] = np.nan\n",
    "                row[f'{prefix}_base_spe'] = np.nan\n",
    "                row[f'{prefix}_type1'] = None\n",
    "                row[f'{prefix}_type2'] = None\n",
    "\n",
    "        all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df\n",
    "\n",
    "# === Usage ===\n",
    "pokemon_db = build_pokemon_database(train_data)\n",
    "\n",
    "train_team_df = process_battle_data_2(train_data, pokemon_db, is_train=True)\n",
    "#test_team_df = process_battle_data_2(test_data, pokemon_db, is_train=False)\n",
    "\n",
    "print(train_team_df.head())\n",
    "print(\"Train team DataFrame shape:\", train_team_df.shape)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# --- Boolean switches ---\n",
    "use_names = False\n",
    "use_stats = False\n",
    "use_types = True\n",
    "use_aggregates = False  # mean/range/std features\n",
    "\n",
    "# --- Identify stats columns ---\n",
    "stat_cols = [c for c in train_team_df.columns if 'pokemon' in c and any(s in c for s in ['base_', 'level'])]\n",
    "\n",
    "# --- Compute aggregates if toggled ---\n",
    "if use_aggregates and stat_cols:\n",
    "    for team in ['p1', 'p2']:\n",
    "        team_stat_cols = [c for c in stat_cols if c.startswith(team)]\n",
    "        if team_stat_cols:\n",
    "            # Combined all stats\n",
    "            stats_arr = train_team_df[team_stat_cols].values\n",
    "            train_team_df[f'{team}_mean_all_stats'] = np.nanmean(stats_arr, axis=1)\n",
    "            train_team_df[f'{team}_std_all_stats'] = np.nanstd(stats_arr, axis=1)\n",
    "            train_team_df[f'{team}_range_all_stats'] = np.nanmax(stats_arr, axis=1) - np.nanmin(stats_arr, axis=1)\n",
    "\n",
    "            # Per-stat aggregates\n",
    "            for stat in ['base_hp','base_atk','base_def','base_spa','base_spd','base_spe','level']:\n",
    "                stat_cols_for_team = [c for c in team_stat_cols if c.endswith(stat)]\n",
    "                if stat_cols_for_team:\n",
    "                    arr = train_team_df[stat_cols_for_team].values\n",
    "                    train_team_df[f'{team}_{stat}_mean'] = np.nanmean(arr, axis=1)\n",
    "                    train_team_df[f'{team}_{stat}_std'] = np.nanstd(arr, axis=1)\n",
    "                    train_team_df[f'{team}_{stat}_range'] = np.nanmax(arr, axis=1) - np.nanmin(arr, axis=1)\n",
    "\n",
    "# --- Collect columns based on switches ---\n",
    "feature_cols = []\n",
    "\n",
    "if use_names:\n",
    "    feature_cols += [c for c in train_team_df.columns if 'pokemon' in c and '_name' in c]\n",
    "if use_stats:\n",
    "    feature_cols += stat_cols\n",
    "if use_types:\n",
    "    feature_cols += [c for c in train_team_df.columns if 'pokemon' in c and '_type' in c]\n",
    "if use_aggregates:\n",
    "    feature_cols += [c for c in train_team_df.columns if c.endswith(('_mean','_std','_range'))]\n",
    "\n",
    "# --- Prepare data ---\n",
    "X_df = train_team_df[feature_cols].fillna(0)  # fill NaNs\n",
    "y = train_team_df['player_won'].values\n",
    "\n",
    "display(X_df.head())\n",
    "\n",
    "# One-hot encode categorical columns (names/types)\n",
    "categorical_cols = [c for c in X_df.columns if 'name' in c or 'type' in c]\n",
    "X_df = pd.get_dummies(X_df, columns=categorical_cols, dummy_na=True)\n",
    "\n",
    "X = X_df.values\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Train simple models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_prob = model.predict_proba(X_val)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Val Accuracy\": acc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": auc\n",
    "    })\n",
    "    print(f\"{name}: Val Acc={acc:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13033998,
     "sourceId": 107555,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
